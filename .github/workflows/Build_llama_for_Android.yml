name: Build llama.cpp for Android

on:
  workflow_dispatch:

jobs:
  build-llama-cpp:
    runs-on: ubuntu-latest
    timeout-minutes: 45

    steps:
      # 1. Free Disk Space
      - name: Free Disk Space
        run: |
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /opt/ghc
          sudo rm -rf /usr/local/share/boost
          sudo docker image prune --all --force

      - name: Checkout Code
        uses: actions/checkout@v4

      # 2. Clone llama.cpp
      - name: Clone llama.cpp
        run: |
          git clone https://github.com/ggerganov/llama.cpp.git
          cd llama.cpp
          git submodule update --init --recursive

      - name: Setup Java 17
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '17'

      - name: Setup Android SDK
        uses: android-actions/setup-android@v3

      - name: Install NDK & CMake
        run: |
          sdkmanager "ndk;27.0.12077973" "cmake;3.22.1"
          sdkmanager "build-tools;34.0.0"

      # 3. Build llama.cpp for Multiple Android ABIs
      - name: Build llama.cpp (arm64-v8a)
        env:
          ANDROID_NDK: ${{ env.ANDROID_HOME }}/ndk/27.0.12077973
        run: |
          cd llama.cpp
          mkdir -p build-android-arm64
          cd build-android-arm64
          
          cmake .. \
            -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake \
            -DANDROID_ABI=arm64-v8a \
            -DANDROID_PLATFORM=android-24 \
            -DANDROID_STL=c++_shared \
            -DCMAKE_BUILD_TYPE=Release \
            -DLLAMA_CUDA=OFF \
            -DLLAMA_VULKAN=OFF \
            -DLLAMA_OPENCL=OFF \
            -DLLAMA_METAL=OFF \
            -DLLAMA_BLAS=OFF \
            -DBUILD_SHARED_LIBS=ON \
            -DLLAMA_BUILD_TESTS=OFF \
            -DLLAMA_BUILD_EXAMPLES=OFF
          
          make -j$(nproc) llama
          
          echo "Built libraries:"
          ls -lh *.so || ls -lh libllama.so

      - name: Build llama.cpp (armeabi-v7a)
        env:
          ANDROID_NDK: ${{ env.ANDROID_HOME }}/ndk/27.0.12077973
        run: |
          cd llama.cpp
          mkdir -p build-android-armv7
          cd build-android-armv7
          
          cmake .. \
            -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake \
            -DANDROID_ABI=armeabi-v7a \
            -DANDROID_PLATFORM=android-24 \
            -DANDROID_STL=c++_shared \
            -DCMAKE_BUILD_TYPE=Release \
            -DLLAMA_CUDA=OFF \
            -DLLAMA_VULKAN=OFF \
            -DLLAMA_OPENCL=OFF \
            -DBUILD_SHARED_LIBS=ON \
            -DLLAMA_BUILD_TESTS=OFF \
            -DLLAMA_BUILD_EXAMPLES=OFF
          
          make -j$(nproc) llama

      - name: Build llama.cpp (x86_64)
        env:
          ANDROID_NDK: ${{ env.ANDROID_HOME }}/ndk/27.0.12077973
        run: |
          cd llama.cpp
          mkdir -p build-android-x86_64
          cd build-android-x86_64
          
          cmake .. \
            -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake \
            -DANDROID_ABI=x86_64 \
            -DANDROID_PLATFORM=android-24 \
            -DANDROID_STL=c++_shared \
            -DCMAKE_BUILD_TYPE=Release \
            -DLLAMA_CUDA=OFF \
            -DLLAMA_VULKAN=OFF \
            -DLLAMA_OPENCL=OFF \
            -DBUILD_SHARED_LIBS=ON \
            -DLLAMA_BUILD_TESTS=OFF \
            -DLLAMA_BUILD_EXAMPLES=OFF
          
          make -j$(nproc) llama

      # 4. Organize Libraries for Android Project
      - name: Organize Android JNI Libraries
        run: |
          mkdir -p dist/jniLibs/arm64-v8a
          mkdir -p dist/jniLibs/armeabi-v7a
          mkdir -p dist/jniLibs/x86_64
          
          # Copy libraries
          cp llama.cpp/build-android-arm64/libllama.so dist/jniLibs/arm64-v8a/
          cp llama.cpp/build-android-armv7/libllama.so dist/jniLibs/armeabi-v7a/
          cp llama.cpp/build-android-x86_64/libllama.so dist/jniLibs/x86_64/
          
          # Also copy libc++_shared.so if needed
          find $ANDROID_NDK/sources/cxx-stl/llvm-libc++/libs/arm64-v8a -name "libc++_shared.so" -exec cp {} dist/jniLibs/arm64-v8a/ \;
          find $ANDROID_NDK/sources/cxx-stl/llvm-libc++/libs/armeabi-v7a -name "libc++_shared.so" -exec cp {} dist/jniLibs/armeabi-v7a/ \;
          find $ANDROID_NDK/sources/cxx-stl/llvm-libc++/libs/x86_64 -name "libc++_shared.so" -exec cp {} dist/jniLibs/x86_64/ \;
          
          # Copy headers for JNI binding
          mkdir -p dist/include
          cp llama.cpp/llama.h dist/include/
          cp llama.cpp/ggml.h dist/include/
          
          echo "Library structure:"
          tree dist/ || find dist/

      # 5. Create JNI Wrapper (Optional - for direct JNI use)
      - name: Create JNI Wrapper Template
        run: |
          mkdir -p dist/jni
          cat <<'EOF' > dist/jni/llama_jni.cpp
          #include <jni.h>
          #include <string>
          #include <android/log.h>
          #include "llama.h"
          
          #define TAG "LlamaCppJNI"
          #define LOGI(...) __android_log_print(ANDROID_LOG_INFO, TAG, __VA_ARGS__)
          #define LOGE(...) __android_log_print(ANDROID_LOG_ERROR, TAG, __VA_ARGS__)
          
          extern "C" {
          
          JNIEXPORT jlong JNICALL
          Java_com_forge_llama_LlamaContext_nativeInit(
              JNIEnv* env,
              jobject /* this */,
              jstring modelPath,
              jint nThreads,
              jint nCtx) {
              
              const char* path = env->GetStringUTFChars(modelPath, nullptr);
              
              llama_model_params model_params = llama_model_default_params();
              llama_model* model = llama_load_model_from_file(path, model_params);
              
              env->ReleaseStringUTFChars(modelPath, path);
              
              if (!model) {
                  LOGE("Failed to load model");
                  return 0;
              }
              
              llama_context_params ctx_params = llama_context_default_params();
              ctx_params.n_ctx = nCtx;
              ctx_params.n_threads = nThreads;
              
              llama_context* ctx = llama_new_context_with_model(model, ctx_params);
              
              if (!ctx) {
                  llama_free_model(model);
                  LOGE("Failed to create context");
                  return 0;
              }
              
              LOGI("Llama context initialized successfully");
              return reinterpret_cast<jlong>(ctx);
          }
          
          JNIEXPORT jstring JNICALL
          Java_com_forge_llama_LlamaContext_nativeGenerate(
              JNIEnv* env,
              jobject /* this */,
              jlong contextPtr,
              jstring prompt,
              jint maxTokens) {
              
              llama_context* ctx = reinterpret_cast<llama_context*>(contextPtr);
              const char* promptStr = env->GetStringUTFChars(prompt, nullptr);
              
              // Tokenize and generate
              // (Simplified - full implementation needed)
              
              std::string result = "Generated text here";
              
              env->ReleaseStringUTFChars(prompt, promptStr);
              return env->NewStringUTF(result.c_str());
          }
          
          JNIEXPORT void JNICALL
          Java_com_forge_llama_LlamaContext_nativeClose(
              JNIEnv* env,
              jobject /* this */,
              jlong contextPtr) {
              
              llama_context* ctx = reinterpret_cast<llama_context*>(contextPtr);
              if (ctx) {
                  llama_model* model = llama_get_model(ctx);
                  llama_free(ctx);
                  if (model) {
                      llama_free_model(model);
                  }
                  LOGI("Llama context freed");
              }
          }
          
          } // extern "C"
          EOF

      # 6. Create Build Instructions
      - name: Create README
        run: |
          cat <<'EOF' > dist/README.md
          # llama.cpp Android Libraries
          
          ## Contents
          - `jniLibs/` - Compiled native libraries for Android
            - `arm64-v8a/` - 64-bit ARM (most modern phones)
            - `armeabi-v7a/` - 32-bit ARM (older devices)
            - `x86_64/` - x86 64-bit (emulators)
          - `include/` - Header files for JNI integration
          - `jni/` - Example JNI wrapper
          
          ## Integration Steps
          
          ### Option 1: Use Java Bindings (Recommended)
          Add to your `build.gradle`:
```gradle
          dependencies {
              implementation 'de.kherud:llama:2.2.0'
          }
```
          
          ### Option 2: Direct JNI (Advanced)
          1. Copy `jniLibs/` to `app/src/main/jniLibs/`
          2. Implement JNI bindings based on `jni/llama_jni.cpp`
          3. Load library: `System.loadLibrary("llama")`
          
          ## Usage with Your Godot Service
          Replace MLC-LLM engine initialization with:

          import de.kherud.llama.LlamaModel
          import de.kherud.llama.ModelParameters
          
          val params = ModelParameters()
              .setNThreads(4)
              .setNCtx(2048)
              .setNGpuLayers(0)  // CPU only
          
          val model = LlamaModel("/path/to/model.gguf", params)
          val response = model.generate("Your prompt", maxTokens = 512)
          model.close()
          
          ## Model Recommendations
          - Phi-2 Q4_K_M (~1.5GB) - Fast, good quality
          - Mistral-7B Q4_K_M (~4GB) - Better quality
          - Qwen2.5-3B Q4_K_M (~2GB) - Balanced
          
          Download from HuggingFace:
          - https://huggingface.co/TheBloke
          - https://huggingface.co/models?search=gguf
          EOF

      # 7. Upload Artifacts
      - name: Upload Libraries
        uses: actions/upload-artifact@v4
        with:
          name: llama-cpp-android
          path: dist/
          
      - name: Display Build Summary
        run: |
          echo "=== Build Complete ==="
          echo "Libraries built for:"
          echo "  - arm64-v8a (64-bit ARM)"
          echo "  - armeabi-v7a (32-bit ARM)"
          echo "  - x86_64 (Emulator)"
          echo ""
          echo "File sizes:"
          du -h dist/jniLibs/*/*.so
