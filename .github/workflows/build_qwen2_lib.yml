name: Build MLC Runtime (Final Fix)

on:
  workflow_dispatch:

jobs:
  build-libs:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      # 1. Clean Disk Space (~25GB free)
      - name: Free Disk Space
        run: |
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /opt/ghc
          sudo rm -rf /usr/local/share/boost
          sudo docker image prune --all --force

      - name: Checkout Code
        uses: actions/checkout@v4

      # 2. Clone Source (Needed for CMake/Android scripts)
      - name: Clone MLC-LLM Source
        run: git clone --recursive https://github.com/mlc-ai/mlc-llm.git mlc-source

      - name: Setup Java 17
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '17'

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      # 3. Install MLC Tool
      - name: Install MLC-LLM Nightly
        run: |
          pip install --pre -U -f https://mlc.ai/wheels mlc-llm-nightly-cpu mlc-ai-nightly-cpu

      # 4. CRITICAL FIX: Robust Python Patch
      # This script finds the "raise ValueError" corresponding to the "Local Path" error
      # and replaces it with "print", effectively disabling the check.
      - name: Patch MLC Validation Logic
        run: |
          python -c "
          import site
          import os

          # Find the package.py file
          package_dir = site.getsitepackages()[0]
          target_path = os.path.join(package_dir, 'mlc_llm', 'interface', 'package.py')
          print(f'Patching file: {target_path}')

          with open(target_path, 'r') as f:
              lines = f.readlines()

          # The specific text in the error message
          error_marker = 'is a local path.Please set'
          patched = False

          for i, line in enumerate(lines):
              if error_marker in line:
                  # Look backwards 10 lines to find the 'raise ValueError' statement
                  for j in range(i, max(-1, i - 10), -1):
                      if 'raise ValueError' in lines[j]:
                          print(f'Found error raise on line {j+1}. Disabling it.')
                          # Replacing 'raise ValueError' with 'print' keeps syntax valid 
                          # (since print is a function and takes the string arguments)
                          lines[j] = lines[j].replace('raise ValueError', 'print')
                          patched = True
                          break
                  if patched:
                      break

          if not patched:
              print('WARNING: Could not locate the specific validation logic to patch.')
          else:
              with open(target_path, 'w') as f:
                  f.writelines(lines)
              print('Successfully patched package.py')
          "

      - name: Setup Android SDK
        uses: android-actions/setup-android@v3

      - name: Install NDK & CMake
        run: sdkmanager "ndk;27.0.12077973" "cmake;3.22.1"

      - name: Setup Rust
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          profile: minimal

      # 5. Download ONLY Configs (No Weights)
      - name: Download Model Configs
        run: |
          mkdir -p models
          python -c "
          import os, requests
          models = {
            'mlc-ai/Qwen2.5-Coder-3B-Instruct-q4f16_1-MLC': 'qwen2.5-coder-3b-instruct',
            'mlc-ai/Qwen3-4B-q4f16_1-MLC': 'qwen3-4b',
            'mlc-ai/Qwen2-Math-7B-Instruct-q4f16_1-MLC': 'qwen2-math-7b-instruct',
            'mlc-ai/Qwen2.5-7B-Instruct-q4f16_1-MLC': 'qwen2.5-7b-instruct',
            'mlc-ai/gemma-3-4b-it-q4f16_1-MLC': 'gemma-3-4b-it',
            'mlc-ai/Llama-3.2-3B-Instruct-q4f16_1-MLC': 'llama-3.2-3b-instruct',
            'mlc-ai/Phi-3.5-mini-instruct-q4f16_1-MLC': 'phi-3.5-mini-instruct',
            'mlc-ai/Phi-3-mini-4k-instruct-q4f16_1-MLC': 'phi-3-mini-4k-instruct'
          }
          files = ['mlc-chat-config.json', 'tokenizer.json', 'tokenizer_config.json', 'ndarray-cache.json']
          for repo, name in models.items():
              path = os.path.join('models', name)
              os.makedirs(path, exist_ok=True)
              print(f'Processing {name}...')
              for f in files:
                  try:
                      r = requests.get(f'https://huggingface.co/{repo}/resolve/main/{f}')
                      if r.status_code == 200:
                          with open(os.path.join(path, f), 'wb') as file: file.write(r.content)
                  except: pass
          "

      # 6. Generate Config pointing to Local Paths
      - name: Create Model Config
        run: |
          cat <<EOF > mlc-package-config.json
          {
            "device": "android",
            "model_list": [
              { "model": "./models/qwen2.5-coder-3b-instruct", "model_id": "qwen2.5-coder-3b-instruct", "estimated_vram_bytes": 3000000000, "bundle_weight": false },
              { "model": "./models/qwen3-4b", "model_id": "qwen3-4b", "estimated_vram_bytes": 3500000000, "bundle_weight": false },
              { "model": "./models/qwen2-math-7b-instruct", "model_id": "qwen2-math-7b-instruct", "estimated_vram_bytes": 4600000000, "bundle_weight": false },
              { "model": "./models/qwen2.5-7b-instruct", "model_id": "qwen2.5-7b-instruct", "estimated_vram_bytes": 4600000000, "bundle_weight": false },
              { "model": "./models/gemma-3-4b-it", "model_id": "gemma-3-4b-it", "estimated_vram_bytes": 3200000000, "bundle_weight": false },
              { "model": "./models/llama-3.2-3b-instruct", "model_id": "llama-3.2-3b-instruct", "estimated_vram_bytes": 2600000000, "bundle_weight": false },
              { "model": "./models/phi-3.5-mini-instruct", "model_id": "phi-3.5-mini-instruct", "estimated_vram_bytes": 2800000000, "bundle_weight": false },
              { "model": "./models/phi-3-mini-4k-instruct", "model_id": "phi-3-mini-4k-instruct", "estimated_vram_bytes": 2800000000, "bundle_weight": false }
            ]
          }
          EOF

      # 7. Compile
      - name: Compile Runtime Libraries
        env:
          MLC_LLM_SOURCE_DIR: ${{ github.workspace }}/mlc-source
          ANDROID_NDK: ${{ env.ANDROID_HOME }}/ndk/27.0.12077973
          TVM_NDK_CC: ${{ env.ANDROID_HOME }}/ndk/27.0.12077973/toolchains/llvm/prebuilt/linux-x86_64/bin/aarch64-linux-android24-clang
        run: |
          python -m mlc_llm package

      - name: Upload Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: mlc-runtime-libs
          path: dist/lib/mlc4j/
