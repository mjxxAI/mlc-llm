name: Build MLC Runtime (Space Optimized)

on:
  workflow_dispatch:

jobs:
  build-libs:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      # 1. MAXIMIZE DISK SPACE (Crucial: Frees ~25GB)
      - name: Free Disk Space
        run: |
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /opt/ghc
          sudo rm -rf /usr/local/share/boost
          sudo rm -rf "$AGENT_TOOLSDIRECTORY"
          sudo docker image prune --all --force

      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Clone MLC-LLM Source
        run: git clone --recursive https://github.com/mlc-ai/mlc-llm.git mlc-source

      - name: Setup Java 17
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '17'

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install MLC-LLM Nightly
        run: |
          pip install --pre -U -f https://mlc.ai/wheels mlc-llm-nightly-cpu mlc-ai-nightly-cpu

      - name: Setup Android SDK
        uses: android-actions/setup-android@v3

      - name: Install NDK & CMake
        run: sdkmanager "ndk;27.0.12077973" "cmake;3.22.1"

      - name: Setup Rust
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          profile: minimal

      # 2. INTELLIGENT DOWNLOAD
      # Instead of letting MLC download 50GB of weights, we download ONLY the config files.
      - name: Download Model Configs Only
        run: |
          mkdir -p models
          
          python -c "
          import os
          import requests
          
          # List of models mapped to your desired folders
          models = {
            'mlc-ai/Qwen2.5-Coder-3B-Instruct-q4f16_1-MLC': 'qwen2.5-coder-3b-instruct',
            'mlc-ai/Qwen3-4B-q4f16_1-MLC': 'qwen3-4b',
            'mlc-ai/Qwen2-Math-7B-Instruct-q4f16_1-MLC': 'qwen2-math-7b-instruct',
            'mlc-ai/Qwen2.5-7B-Instruct-q4f16_1-MLC': 'qwen2.5-7b-instruct',
            'mlc-ai/gemma-3-4b-it-q4f16_1-MLC': 'gemma-3-4b-it',
            'mlc-ai/Llama-3.2-3B-Instruct-q4f16_1-MLC': 'llama-3.2-3b-instruct',
            'mlc-ai/Phi-3.5-mini-instruct-q4f16_1-MLC': 'phi-3.5-mini-instruct',
            'mlc-ai/Phi-3-mini-4k-instruct-q4f16_1-MLC': 'phi-3-mini-4k-instruct'
          }
          
          files_to_fetch = ['mlc-chat-config.json', 'tokenizer.json', 'tokenizer_config.json', 'ndarray-cache.json']
          
          for repo, local_name in models.items():
              print(f'Processing {local_name}...')
              local_path = os.path.join('models', local_name)
              os.makedirs(local_path, exist_ok=True)
              
              for filename in files_to_fetch:
                  url = f'https://huggingface.co/{repo}/resolve/main/{filename}'
                  try:
                      r = requests.get(url)
                      if r.status_code == 200:
                          with open(os.path.join(local_path, filename), 'wb') as f:
                              f.write(r.content)
                          print(f'  Downloaded {filename}')
                      else:
                          print(f'  Skipped {filename} (Not found or error)')
                  except Exception as e:
                      print(f'  Failed {filename}: {e}')
          "

      # 3. GENERATE CONFIG POINTING TO LOCAL FOLDERS
      # Note: 'model' points to './models/...' so the builder doesn't try to download from HF
      - name: Create Package Config
        run: |
          cat <<EOF > mlc-package-config.json
          {
            "device": "android",
            "model_list": [
              { "model": "./models/qwen2.5-coder-3b-instruct", "model_id": "qwen2.5-coder-3b-instruct", "estimated_vram_bytes": 3000000000, "bundle_weight": false },
              { "model": "./models/qwen3-4b", "model_id": "qwen3-4b", "estimated_vram_bytes": 3500000000, "bundle_weight": false },
              { "model": "./models/qwen2-math-7b-instruct", "model_id": "qwen2-math-7b-instruct", "estimated_vram_bytes": 4600000000, "bundle_weight": false },
              { "model": "./models/qwen2.5-7b-instruct", "model_id": "qwen2.5-7b-instruct", "estimated_vram_bytes": 4600000000, "bundle_weight": false },
              { "model": "./models/gemma-3-4b-it", "model_id": "gemma-3-4b-it", "estimated_vram_bytes": 3200000000, "bundle_weight": false },
              { "model": "./models/llama-3.2-3b-instruct", "model_id": "llama-3.2-3b-instruct", "estimated_vram_bytes": 2600000000, "bundle_weight": false },
              { "model": "./models/phi-3.5-mini-instruct", "model_id": "phi-3.5-mini-instruct", "estimated_vram_bytes": 2800000000, "bundle_weight": false },
              { "model": "./models/phi-3-mini-4k-instruct", "model_id": "phi-3-mini-4k-instruct", "estimated_vram_bytes": 2800000000, "bundle_weight": false }
            ]
          }
          EOF

      - name: Compile Runtime Libraries
        env:
          MLC_LLM_SOURCE_DIR: ${{ github.workspace }}/mlc-source
          ANDROID_NDK: ${{ env.ANDROID_HOME }}/ndk/27.0.12077973
          TVM_NDK_CC: ${{ env.ANDROID_HOME }}/ndk/27.0.12077973/toolchains/llvm/prebuilt/linux-x86_64/bin/aarch64-linux-android24-clang
        run: |
          python -m mlc_llm package

      - name: Upload Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: mlc-runtime-libs
          path: dist/lib/mlc4j/
