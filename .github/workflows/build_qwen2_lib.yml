name: Build MLC Runtime (.so)

on:
  workflow_dispatch: # Allows you to click "Run" manually

jobs:
  build-libs:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Setup Java 17
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '17'

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      # Install Nightly to support Gemma-3 / Qwen-3
      - name: Install MLC-LLM Nightly
        run: |
          pip install --pre -U -f https://mlc.ai/wheels mlc-llm-nightly-cpu mlc-ai-nightly-cpu
          python -c "import mlc_llm; print('MLC installed version:', mlc_llm.__version__)"

      - name: Setup Android NDK
        # Standard GitHub runners have NDK, but we ensure the path is set
        uses: android-actions/setup-android@v3

      - name: Install NDK & CMake
        run: sdkmanager "ndk;27.0.12077973" "cmake;3.22.1"

      - name: Setup Rust
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          profile: minimal

      # This step injects your SPECIFIC 8 models into the config
      - name: Create Model Config
        run: |
          cat <<EOF > mlc-package-config.json
          {
            "device": "android",
            "model_list": [
              {
                "model": "HF://mlc-ai/Qwen2.5-Coder-3B-Instruct-q4f16_1-MLC",
                "model_id": "qwen2.5-coder-3b-instruct",
                "estimated_vram_bytes": 3000000000,
                "bundle_weight": false
              },
              {
                "model": "HF://mlc-ai/Qwen3-4B-q4f16_1-MLC",
                "model_id": "qwen3-4b",
                "estimated_vram_bytes": 3500000000,
                "bundle_weight": false
              },
              {
                "model": "HF://mlc-ai/Qwen2-Math-7B-Instruct-q4f16_1-MLC",
                "model_id": "qwen2-math-7b-instruct",
                "estimated_vram_bytes": 4600000000,
                "bundle_weight": false
              },
              {
                "model": "HF://mlc-ai/Qwen2.5-7B-Instruct-q4f16_1-MLC",
                "model_id": "qwen2.5-7b-instruct",
                "estimated_vram_bytes": 4600000000,
                "bundle_weight": false
              },
              {
                "model": "HF://mlc-ai/gemma-3-4b-it-q4f16_1-MLC",
                "model_id": "gemma-3-4b-it",
                "estimated_vram_bytes": 3200000000,
                "bundle_weight": false
              },
              {
                "model": "HF://mlc-ai/Llama-3.2-3B-Instruct-q4f16_1-MLC",
                "model_id": "llama-3.2-3b-instruct",
                "estimated_vram_bytes": 2600000000,
                "bundle_weight": false
              },
              {
                "model": "HF://mlc-ai/Phi-3.5-mini-instruct-q4f16_1-MLC",
                "model_id": "phi-3.5-mini-instruct",
                "estimated_vram_bytes": 2800000000,
                "bundle_weight": false
              },
              {
                "model": "HF://mlc-ai/Phi-3-mini-4k-instruct-q4f16_1-MLC",
                "model_id": "phi-3-mini-4k-instruct",
                "estimated_vram_bytes": 2800000000,
                "bundle_weight": false
              }
            ]
          }
          EOF

      - name: Compile Runtime Libraries (.so)
        env:
          ANDROID_NDK: ${{ env.ANDROID_HOME }}/ndk/27.0.12077973
          TVM_NDK_CC: ${{ env.ANDROID_HOME }}/ndk/27.0.12077973/toolchains/llvm/prebuilt/linux-x86_64/bin/aarch64-linux-android24-clang
        run: |
          # This command compiles the C++ kernels for all 8 models into one library
          python -m mlc_llm package

      # Upload the JNI libraries (.so) and the Generated Java code
      - name: Upload Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: mlc-runtime-libs
          path: dist/lib/mlc4j/
