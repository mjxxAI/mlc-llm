name: Build llama.cpp for Android - Yoga Pad Pro AI (Snapdragon 8 Gen 3)

on:
  workflow_dispatch:
    inputs:
      thermal_profile:
        description: 'Thermal optimization profile'
        required: true
        default: 'balanced'
        type: choice
        options:
          - 'peak'
          - 'balanced'
          - 'efficiency'

jobs:
  build-llama-cpp:
    runs-on: ubuntu-latest
    timeout-minutes: 90

    steps:
      - name: Free Disk Space
        run: |
          sudo rm -rf /usr/share/dotnet /opt/ghc /usr/local/share/boost
          sudo rm -rf /usr/local/share/chromium /usr/local/share/powershell
          sudo rm -rf /usr/share/swift /opt/hostedtoolcache/CodeQL "$AGENT_TOOLSDIRECTORY"
          sudo docker image prune --all --force
          df -h

      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Clone llama.cpp
        run: |
          git clone --depth 1 https://github.com/ggerganov/llama.cpp.git
          cd llama.cpp
          git submodule update --init --recursive

      - name: Setup Java 17
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '17'

      - name: Setup Android SDK
        uses: android-actions/setup-android@v3

      - name: Install NDK & CMake
        run: |
          sdkmanager "ndk;27.0.12077973" "cmake;3.22.1" "platform-tools" "build-tools;34.0.0"
          echo "ANDROID_NDK=${ANDROID_HOME}/ndk/27.0.12077973" >> $GITHUB_ENV

      - name: Build llama.cpp (ARMv9 - Snapdragon 8 Gen 3)
        run: |
          case "${{ github.event.inputs.thermal_profile }}" in
            peak)
              THERMAL_FLAGS="-mtune=cortex-x4 -fomit-frame-pointer -funroll-all-loops"
              ;;
            efficiency)
              THERMAL_FLAGS="-mtune=cortex-a520 -Os -finline-limit=64"
              ;;
            *)
              THERMAL_FLAGS="-mtune=cortex-a720 -fomit-frame-pointer -funroll-loops"
              ;;
          esac
          
          export CXXFLAGS="-march=armv9-a+fp16+dotprod+i8mm+bf16 ${THERMAL_FLAGS} -O3 -flto=thin -ffunction-sections -fdata-sections -fvisibility=hidden -D__ARM_FEATURE_MATMUL_INT8=1 -D__ARM_FEATURE_BF16=1"
          export CFLAGS="-march=armv9-a+fp16+dotprod+i8mm+bf16 ${THERMAL_FLAGS} -O3 -flto=thin -ffunction-sections -fdata-sections -D__ARM_FEATURE_MATMUL_INT8=1 -D__ARM_FEATURE_BF16=1"
          
          cd llama.cpp
          mkdir -p build-armv9
          cd build-armv9
          
          cmake .. \
            -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake \
            -DANDROID_ABI=arm64-v8a \
            -DANDROID_PLATFORM=android-24 \
            -DANDROID_STL=c++_shared \
            -DCMAKE_BUILD_TYPE=Release \
            -DLLAMA_NATIVE=OFF \
            -DLLAMA_CPU_ARM_ARCH=native \
            -DLLAMA_ARM_NEON=ON \
            -DLLAMA_ARM_DOTPROD=ON \
            -DLLAMA_ARM_I8MM=ON \
            -DLLAMA_FLASH_ATTN=ON \
            -DLLAMA_CUDA=OFF \
            -DLLAMA_VULKAN=OFF \
            -DLLAMA_OPENCL=OFF \
            -DLLAMA_METAL=OFF \
            -DLLAMA_BLAS=OFF \
            -DLLAMA_OPENMP=OFF \
            -DBUILD_SHARED_LIBS=ON \
            -DLLAMA_BUILD_TESTS=OFF \
            -DLLAMA_BUILD_EXAMPLES=OFF \
            -DLLAMA_BUILD_SERVER=OFF
          
          make -j$(nproc) llama
          echo "=== ARMv9 Build Complete ==="
          ls -lh bin/libllama.so

      - name: Build llama.cpp (ARMv8 - Fallback)
        run: |
          export CXXFLAGS="-march=armv8.2-a+fp16+dotprod -mtune=cortex-a77 -O3 -flto=thin -ffunction-sections -fdata-sections"
          export CFLAGS="-march=armv8.2-a+fp16+dotprod -mtune=cortex-a77 -O3 -flto=thin -ffunction-sections -fdata-sections"
          
          cd llama.cpp
          mkdir -p build-armv8
          cd build-armv8
          
          cmake .. \
            -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake \
            -DANDROID_ABI=arm64-v8a \
            -DANDROID_PLATFORM=android-24 \
            -DANDROID_STL=c++_shared \
            -DCMAKE_BUILD_TYPE=Release \
            -DLLAMA_NATIVE=OFF \
            -DLLAMA_CPU_ARM_ARCH=armv8.2-a \
            -DLLAMA_ARM_NEON=ON \
            -DLLAMA_ARM_DOTPROD=ON \
            -DLLAMA_ARM_I8MM=OFF \
            -DLLAMA_FLASH_ATTN=ON \
            -DLLAMA_CUDA=OFF \
            -DLLAMA_VULKAN=OFF \
            -DLLAMA_OPENCL=OFF \
            -DLLAMA_METAL=OFF \
            -DLLAMA_BLAS=OFF \
            -DLLAMA_OPENMP=OFF \
            -DBUILD_SHARED_LIBS=ON \
            -DLLAMA_BUILD_TESTS=OFF \
            -DLLAMA_BUILD_EXAMPLES=OFF \
            -DLLAMA_BUILD_SERVER=OFF
          
          make -j$(nproc) llama
          echo "=== ARMv8 Build Complete ==="
          ls -lh bin/libllama.so

      - name: Verify ARMv9 Build
        run: |
          cd llama.cpp/build-armv9
          echo "=== ARMv9 Instructions Check ==="
          $ANDROID_NDK/toolchains/llvm/prebuilt/linux-x86_64/bin/llvm-objdump -d bin/libllama.so | \
            grep -E "(sdot|udot|smmla|ummla)" | head -5 || \
            echo "Instructions found (may be inlined)"

      - name: Organize Libraries
        run: |
          mkdir -p dist/jniLibs/arm64-v8a
          mkdir -p dist/include
          
          # Copy ARMv9 optimized build
          cp llama.cpp/build-armv9/bin/libllama.so dist/jniLibs/arm64-v8a/libllama.so-armv9
          
          # Copy ARMv8 fallback build (as default)
          cp llama.cpp/build-armv8/bin/libllama.so dist/jniLibs/arm64-v8a/libllama.so-armv8
          cp llama.cpp/build-armv8/bin/libllama.so dist/jniLibs/arm64-v8a/libllama.so
          
          # Copy NDK STL
          NDK_LIB="$ANDROID_NDK/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/lib/arm64-v8a/libc++_shared.so"
          [ -f "$NDK_LIB" ] && cp "$NDK_LIB" dist/jniLibs/arm64-v8a/
          
          # Copy headers
          find llama.cpp -name "llama.h" -maxdepth 3 -exec cp {} dist/include/ \;
          find llama.cpp -name "ggml.h" -maxdepth 3 -exec cp {} dist/include/ \;
          
          # Create detection script
          cat > dist/jniLibs/arm64-v8a/README.txt <<'EOF'
          Library Selection:
          - libllama.so : Default ARMv8.2 (broad compatibility)
          - libllama.so-armv9 : Optimized for Snapdragon 8 Gen 3+ (ARMv9)
          
          Auto-detection (Kotlin):
          val libName = if (isSnapdragon8Gen3()) "llama-armv9" else "llama"
          System.loadLibrary(libName)
          EOF
          
          ls -lh dist/jniLibs/arm64-v8a/

      - name: Upload Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: llama-cpp-android-armv9-snapdragon8g3
          path: dist/
          retention-days: 30

      - name: Display Summary
        run: |
          echo "=========================================="
          echo "  Yoga Pad Pro AI (Snapdragon 8 Gen 3)   "
          echo "  llama.cpp Build Complete               "
          echo "=========================================="
          echo ""
          echo "Profile: ${{ github.event.inputs.thermal_profile }}"
          ls -lh dist/jniLibs/arm64-v8a/*.so
          echo ""
          echo "ARMv9: $(test -f dist/jniLibs/arm64-v8a/libllama.so-armv9 && echo '✓' || echo '✗')"
          echo "ARMv8: $(test -f dist/jniLibs/arm64-v8a/libllama.so-armv8 && echo '✓' || echo '✗')"
          echo "=========================================="
